{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shadi159/Cloud-Computing/blob/main/Copy_of_cloudSping2025tut6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voQt1cu0ce-h"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def fetch_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "            \"Mozilla/5.0\"\n",
        "        )\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print(\"HTTP status:\", response.status_code)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            return soup\n",
        "        else:\n",
        "            print(\"Response head:\", response.text[:200])\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(\"Request error:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def index_words(soup):\n",
        "  index = {}\n",
        "  words = re.findall(r'\\w+', soup.get_text())\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in index:\n",
        "      index[word] += 1\n",
        "    else:\n",
        "      index[word] = 1\n",
        "  return index"
      ],
      "metadata": {
        "id": "z_oG5HYzc0dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at'}\n",
        "  for stop_word in stop_words:\n",
        "    if stop_word in index:\n",
        "      del index[stop_word]\n",
        "  return index"
      ],
      "metadata": {
        "id": "6Qb9vL0rdPyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index"
      ],
      "metadata": {
        "id": "hKG3m2TOdYTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results"
      ],
      "metadata": {
        "id": "qYFBlAu5dpQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(url, query):\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "     return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  results = search(query, index)\n",
        "  return results"
      ],
      "metadata": {
        "id": "UJ7bA3_bd302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'bird'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "7cm584qoeFlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "8E3z42zqeZSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  stemmer = PorterStemmer()\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    word = stemmer.stem(word)\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results"
      ],
      "metadata": {
        "id": "lK_fHt8Aebwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "wUBSB66JfHQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n"
      ],
      "metadata": {
        "id": "izmK0W0TfT2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "   rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "id": "OrF-H-FCfcB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'collage students'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n"
      ],
      "metadata": {
        "id": "5gCceVI1fhVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'owls'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "id": "1NGHh_Trfz1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'Industry'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "id": "3AzhTcwKnuZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### **מנוע המיועד למספר דפים**"
      ],
      "metadata": {
        "id": "NNHQfDngf-QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "class WikiSearchEngine:\n",
        "  def __init__(self):\n",
        "    \"\"\"Initialize the search engine\"\"\"\n",
        "    self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    self.pages = []\n",
        "    self.word_locations = defaultdict(list) # word -> [(page_id, frequency), ...]\n",
        "    self.stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "    return False\n",
        "  def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "    \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "    search_params = {\n",
        "      \"action\": \"query\",\n",
        "      \"format\": \"json\",\n",
        "      \"list\": \"search\",\n",
        "      \"srsearch\": topic,\n",
        "      \"srlimit\": num_pages\n",
        "    }\n",
        "    try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "      print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error fetching pages: {str(e)}\")\n",
        "\n",
        "  def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "  def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                    if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "gK6qrTDSf91I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "  \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "  search_params = {\n",
        "  \"action\": \"query\",\n",
        "  \"format\": \"json\",\n",
        "  \"list\": \"search\",\n",
        "  \"srsearch\": topic,\n",
        "  \"srlimit\": num_pages\n",
        "  }\n",
        "  try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "        print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching pages: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "XvdwliQ7hEhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                      if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results"
      ],
      "metadata": {
        "id": "F-5SwSt6jv3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Independent Work**"
      ],
      "metadata": {
        "id": "kXlkT8ILUrXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class WebSearchEngine:\n",
        "    def __init__(self):\n",
        "        \"\"\"Simple search engine over arbitrary web pages.\"\"\"\n",
        "        self.pages = []\n",
        "        self.word_locations = defaultdict(list)\n",
        "        self.stop_words = {\n",
        "            'a', 'an', 'the', 'and', 'or', 'in', 'on',\n",
        "            'at', 'to', 'for', 'of', 'with', 'is', 'are', 'this', 'that',\n",
        "            'as', 'by', 'from', 'it', 'be', 'was', 'were', 'has', 'have'\n",
        "        }\n",
        "\n",
        "    def _extract_text_from_html(self, html):\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=\" \")\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def add_page_from_url(self, url, page_id=None):\n",
        "        try:\n",
        "            print(f\"Fetching: {url}\")\n",
        "            resp = requests.get(url, timeout=15)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            html = resp.text\n",
        "            content = self._extract_text_from_html(html)\n",
        "\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            title_tag = soup.find(\"title\")\n",
        "            title = title_tag.get_text(strip=True) if title_tag else url\n",
        "\n",
        "            if page_id is None:\n",
        "                page_id = len(self.pages) + 1\n",
        "\n",
        "            self.pages.append({\n",
        "                'id': page_id,\n",
        "                'title': title,\n",
        "                'url': url,\n",
        "                'content': content\n",
        "            })\n",
        "            print(f\"Added page: {title}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def add_pages_from_urls(self, urls):\n",
        "        for i, url in enumerate(urls, start=1):\n",
        "            self.add_page_from_url(url, page_id=i)\n",
        "\n",
        "    def build_index(self):\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        for page in self.pages:\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "    def get_context(self, text, query_words, window=40):\n",
        "        text_lower = text.lower()\n",
        "        for qw in query_words:\n",
        "            idx = text_lower.find(qw)\n",
        "            if idx != -1:\n",
        "                start = max(0, idx - window)\n",
        "                end = min(len(text), idx + len(qw) + window)\n",
        "                snippet = text[start:end]\n",
        "                return \" \".join(snippet.split())\n",
        "        return \" \".join(text[:2 * window].split())\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "        query_words = [\n",
        "            w.lower()\n",
        "            for w in re.findall(r'\\w+', query)\n",
        "            if w.lower() not in self.stop_words\n",
        "        ]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        for word in query_words:\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "        ranked = [\n",
        "            (pid, info['matches'], info['total_freq'])\n",
        "            for pid, info in page_scores.items()\n",
        "        ]\n",
        "        ranked.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "GqvtIssMPraa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "urls = [\n",
        "    \"https://link.springer.com/article/10.1007/s10462-024-11100-x\",\n",
        "    \"https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2016.01419/full\",\n",
        "    \"https://www.nature.com/articles/s41598-025-05102-0\",\n",
        "    \"https://www.planthealthcentre.scot/plant-diseases/resources\"\n",
        "]\n",
        "\n",
        "KEYWORDS = [\n",
        "    \"plant\",\n",
        "    \"disease\",\n",
        "    \"leaf\",\n",
        "    \"detection\",\n",
        "    \"color\",\n",
        "    \"Vector\",\n",
        "    \"Wilt\",\n",
        "    \"Blight\",\n",
        "    \"Lesion\",\n",
        "    \"Identifying\"\n",
        "]\n",
        "\n",
        "engine = WebSearchEngine()\n",
        "\n",
        "engine.add_pages_from_urls(urls)\n",
        "\n",
        "engine.build_index()\n",
        "\n",
        "index_data = {\n",
        "    \"pages\": {},\n",
        "    \"words\": {}\n",
        "}\n",
        "\n",
        "for page in engine.pages:\n",
        "    index_data[\"pages\"][str(page[\"id\"])] = {\n",
        "        \"title\": page[\"title\"],\n",
        "        \"url\": page[\"url\"]\n",
        "    }\n",
        "\n",
        "for kw in KEYWORDS:\n",
        "    w = kw.lower()\n",
        "    word_info = {}\n",
        "    for page_id, freq in engine.word_locations.get(w, []):\n",
        "        word_info[str(page_id)] = freq\n",
        "\n",
        "    index_data[\"words\"][w] = word_info\n",
        "\n",
        "print(index_data)\n"
      ],
      "metadata": {
        "id": "C2sS6SZCQy9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIREBASE_URL = \"https://identifying-plant-diseas-ef19d-default-rtdb.firebaseio.com/\"\n",
        "\n",
        "def save_index_to_firebase(index_data, path=\"plant_disease_index\"):\n",
        "    url = f\"{FIREBASE_URL}/{path}.json\"\n",
        "\n",
        "    try:\n",
        "        response = requests.put(url, json=index_data)\n",
        "        response.raise_for_status()\n",
        "        print(\"Saved to Firebase successfully!\")\n",
        "        print(\"Response:\", response.json())\n",
        "    except Exception as e:\n",
        "        print(\"Error saving to Firebase:\", e)\n",
        "\n",
        "save_index_to_firebase(index_data)\n"
      ],
      "metadata": {
        "id": "5H0nkII-RFHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_keyword_freq_for_page(engine, page_id, keywords):\n",
        "    freqs = []\n",
        "    for kw in keywords:\n",
        "        kw = kw.lower()\n",
        "        freq = 0\n",
        "        for pid, f in engine.word_locations.get(kw, []):\n",
        "            if pid == page_id:\n",
        "                freq = f\n",
        "                break\n",
        "        freqs.append(freq)\n",
        "    return freqs\n",
        "\n",
        "for page in engine.pages:\n",
        "    page_id = page[\"id\"]\n",
        "    title = page[\"title\"]\n",
        "\n",
        "    freqs = get_keyword_freq_for_page(engine, page_id, KEYWORDS)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.bar(KEYWORDS, freqs)\n",
        "    plt.title(f\"Keyword frequencies in page: {title}\")\n",
        "    plt.xlabel(\"Keyword\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qCwTjBEUTuZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}